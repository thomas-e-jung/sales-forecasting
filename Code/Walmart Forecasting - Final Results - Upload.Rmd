---
title: "Sales Forecasting - Final Results"
author: "Thomas Jung"
output:
  html_document:
    df_print: paged
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
# install.packages("corrgram")
# install.packages("car")
# install.packages("lubridate")
# install.packages("plyr")
# install.packages("tidyverse")
# install.packages("xgboost")
# install.packages("caret")
# install.packages("car")
# install.packages("mctest")
# install.packages("DAAG")
# install.packages("moments")
# install.packages("prophet")
# install.packages("data.table")
# install.packages("nortest")


library(corrgram)
library(car)
library(lubridate)
library(plyr)
library(tidyverse)
library(xgboost)
library(caret)
library(car)
library(mctest)
library(DAAG)
library(moments)
library(prophet)
library(data.table)
library(nortest)
```


Exploratory Data Analysis

Stores Data
```{r}
stores_raw <- read.csv("stores.csv")
stores_raw$Store <- as.factor(stores_raw$Store)
str(stores_raw)
summary(stores_raw)

# outliers
Boxplot(Size~Type, data = stores_raw, id.method = "y")
stores_raw[c(33, 36, 3, 5),]
stores_clean <- stores_raw
stores_clean %>%
  group_by(Type) %>%
  summarize(Average_size = mean(Size), .groups = "keep")

# replace outliers to Type "C" since values are similar to the mean of "C" stores
stores_clean[c(33, 36, 3, 5),2] <- "C"


# check NAs
sum(is.na(stores_clean))
summary(stores_clean)
```

Training Data
```{r}
train_raw <- read.csv("train.csv")
str(train_raw)
summary(train_raw)
train_clean <- train_raw
train_clean$Store <- as.factor(train_clean$Store)
train_clean$Dept <- as.factor(train_clean$Dept)

# check missing depts in each store
stores_unique <- levels(train_clean$Store)
depts_unique <- levels(train_clean$Dept)
stores_list <- as.list(stores_unique)
check_depts <- function(x) {
  identical(levels(train_clean[train_clean$Store == x, 2]), depts_unique)
}
summary(sapply(stores_list, check_depts))

# check missing dates
train_clean$Date <- as.Date(train_clean$Date)
range(train_clean$Date)
dates_unique <- unique(train_clean$Date)
dates_generated <- seq(min(train_clean$Date), max(train_clean$Date), by = "week")
identical(dates_unique, dates_generated)



# outliers; not removed due to most outliers being around Christmas, Black Friday, and Super Bowl
# extreme data captured by IsHoliday
Boxplot(train_clean$Weekly_Sales)
train_clean[95374,]

train_clean %>%
    filter(Store == 10 & Dept == 72) %>%
    arrange(desc(Weekly_Sales)) %>%
    head(10)

# check NAs
sum(is.na(train_clean))

train_clean$week_number <- week(train_clean$Date)

summary(train_clean)
str(train_clean)
```

Features Data
```{r}
features_raw <- read.csv("features.csv")
str(features_raw)
features_clean <- features_raw
features_clean$Store <- as.factor(features_clean$Store)
features_clean$Date  <- as.Date(features_clean$Date)
summary(features_clean)

range(features_clean$Date)
range(max(train_clean$Date), max(features_clean$Date))

# check NAs; CPI, Unemployment have 585 NAs each corresponding to 13 extra weeks * 45 stores = 585, not found in training data; disregard
table(is.na(features_clean))
sum(is.na(features_clean$CPI))
sum(is.na(features_clean$Unemployment))

# remove MarkDowns; too many NAs
features_clean <- features_clean[, c(1:4, 10:12)]
str(features_clean)
summary(features_clean)

# Outliers
Boxplot(features_clean[3:6])
```

Combine datasets
```{r}
df <- plyr::join(train_clean, stores_clean, by = "Store", type = "left")
df <- plyr::join(df, features_clean, by = c("Store", "Date"), type = "left")
identical(df[5], df[13])
df[13] <- NULL
```


Multiple Regression
```{r}
# Remove Datetime
df_reg <- df %>%
  dplyr::select(-Date)

# Check for skewness
skewness(df_reg$Weekly_Sales)
# skewness = 3.26; right skewed

# Check for normality
ad.test(df_reg$Weekly_Sales)
# p-value <2e-16; non-normal distribution

# Check for non-constant variance (heteroskedasticity) & non-normality
model_1 <- lm(Weekly_Sales ~ ., data = df_reg)
summary(model_1)
# R2 = 0.658,
# p-value < 2e-16
par(mfrow=c(2,2))
plot(model_1)
# heteroskedasticity present; non-normal distribution

# Check for correlations between independent variables
X <- df_reg %>%
  dplyr::select(-Weekly_Sales)
Y <- df_reg %>%
  dplyr::select(Weekly_Sales)
corrgram(X, order=TRUE, lower.panel=panel.shade,
         upper.panel=panel.pie, text.panel=panel.txt,
         )
# Moderate correlations:
# Fuel price & temperature
# Temperature & week_number
# Temperature & CPI
# Unemployment & CPI

# Check for multicollinearity using variance inflation factor
vif(model_1)
# Check variables that are linearly dependent
attributes(alias(model_1)$Complete)$dimnames[[1]]
model_1 <- lm(Weekly_Sales ~ .-Type, data = df_reg)
attributes(alias(model_1)$Complete)$dimnames[[1]]
model_1 <- lm(Weekly_Sales ~ .-Type -Size, data = df_reg)
vif(model_1)
model_1 <- lm(Weekly_Sales ~ .-Type -Size -CPI, data = df_reg)
vif(model_1)

model_2 <- lm(Weekly_Sales ~ .-Type -Size -CPI, data = df_reg)
summary(model_2)
# R2 = 0.658,
# p-value < 2e-16
# no improvement over model_1

df_reg <- df_reg %>%
  dplyr::select(-c(Type, Size, CPI))



# Train & Test sets (train on first 24 months, reserve last 9 months for testing)
rn_train <- as.numeric(rownames(df[df$Date < as.Date("2012-02-04"),]))
train_set <- df_reg[rn_train,]
test_set <- df_reg[-rn_train,]

# Normalize data (z-normalization)
Weekly_Sales_sd <- sd(train_set$Weekly_Sales)
Weekly_Sales_mean <- mean(train_set$Weekly_Sales)
train_set <- train_set %>%
  mutate(Weekly_Sales_Norm = (Weekly_Sales - Weekly_Sales_mean)/Weekly_Sales_sd) %>%
  dplyr::select(-Weekly_Sales)
test_set <- test_set %>%
  mutate(Weekly_Sales_Norm = (Weekly_Sales - Weekly_Sales_mean)/Weekly_Sales_sd) %>%
  dplyr::select(-Weekly_Sales)

# Train model
model <- lm(Weekly_Sales_Norm ~ ., data = train_set)
summary(model)
# R2: 0.644
# p-value: <2e-16

prediction <- predict(model, newdata = test_set)

# Denormalize data
output <- data.frame(prediction_norm = prediction, actual_norm = test_set$Weekly_Sales_Norm) %>%
  mutate(prediction = (prediction_norm*Weekly_Sales_sd)+Weekly_Sales_mean, actual = (actual_norm*Weekly_Sales_sd)+Weekly_Sales_mean) %>%
  dplyr::select(c("prediction", "actual"))


# Errors
errors <- output$prediction - output$actual
hist(errors)

# Root Mean Squared Error
pred_rmse <- RMSE(output$prediction, output$actual)
paste("RMSE:", pred_rmse)
# RMSE: 12229.0139946726

# Percentage of cases with less than 30% error
rel_change <- 1 - ((output$actual - abs(errors)) / output$actual)
pred30 <- table(rel_change<0.30)["TRUE"] / nrow(output)
paste("PRED(30):", pred30)
# PRED(30): 0.307341201126
```

XGBoost
```{r}
head(df)

# Isolate dependent variable from dataset
salesInfo_salesRemoved <- df %>%
  dplyr::select(-c(Weekly_Sales, Date)) %>%
  mutate_if(is.factor, as.character) %>%
  mutate_if(is.logical, as.numeric)

salesValues <- df$Weekly_Sales

salesInfo_numeric <- salesInfo_salesRemoved %>%
  select_if(is.numeric)

str(salesInfo_numeric)

# One-hot encoding for categorical variables

options(na.action = "na.pass")
storeMatrix <- model.matrix(~Store - 1, df)
deptMatrix <- model.matrix(~Dept - 1, df)
# leave date features (week_number) as numeric
typeMatrix <- model.matrix(~Type - 1, df)
salesInfo_numeric <- cbind(salesInfo_numeric, storeMatrix, deptMatrix, typeMatrix)
salesInfo_matrix <- data.matrix(salesInfo_numeric)

# Train & Test sets (train on first 24 months, reserve last 9 months for testing)
range(df$Date)
rn_train <- as.numeric(rownames(df[df$Date < as.Date("2012-02-04"),]))
train_data <- salesInfo_matrix[rn_train,]
train_values <- salesValues[rn_train]
test_data <- salesInfo_matrix[-rn_train,]
test_values <- salesValues[-rn_train]

# Dmatrix objects
dtrain <- xgb.DMatrix(data = train_data, label = train_values)
dtest <- xgb.DMatrix(data = test_data, label = test_values)

# Tune XGBoost using 5-fold cross-validation
# params <- list(booster = "gbtree", objective = "reg:squarederror", eta=0.3, gamma=0, max_depth=6, min_child_weight=1, subsample=1, colsample_bytree=1)
# xgbcv <- xgb.cv(params = params, data = dtrain, nrounds = 500, nfold = 5, showsd = T, stratified = T, print_every_n = 10, early_stopping_rounds = 10, maximize = F)
# min(xgbcv$evaluation_log$test_rmse_mean)
# RMSE continues to decrease even at 500 rounds

# Train XGBoost model
xgb_model <- xgboost(data = dtrain,
                 nrounds = 500,
                 print_every_n = 10,
                 early_stopping_rounds = 5
                 )
                  
# Test xgb_model
pred_values <- predict(xgb_model, dtest)
rmse = caret::RMSE(test_values, pred_values)
paste("RMSE: ", rmse)
# RMSE: 4215.517 similar to round 211

# tune nrounds = 211
xgb_model_2 <- xgboost(data = dtrain,
                 nrounds = 211,
                 print_every_n = 10,
                 early_stopping_rounds = 5
                 )

# Test xgb_model_2
pred_values <- predict(xgb_model_2, dtest)
rmse = caret::RMSE(test_values, pred_values)
paste("RMSE: ", rmse)
# RMSE: 4617.917 (worse than xgb_model)

# tune nrounds = 1000
xgb_model_3 <- xgboost(data = dtrain,
                 nrounds = 1000,
                 print_every_n = 100,
                 early_stopping_rounds = 5
                 )

# Test xgb_model_3
pred_values <- predict(xgb_model_3, dtest)
rmse = caret::RMSE(test_values, pred_values)
paste("RMSE: ", rmse)
# RMSE: 4086.292 (best of 3 models)

# Root Mean Squared Error & Percentage of cases with less than 30% error
errors <- pred_values - test_values
rel_change <- 1 - ((test_values - abs(errors)) / test_values)
pred30 <- table(rel_change<0.30)["TRUE"] / length(test_values)
paste("PRED(30):", pred30)
# PRED(30): 0.659047517560451

# Output results as CSV
output_df_results <- df %>%
    filter(Date > as.Date("2012-02-04")) %>%
    dplyr::select(1:4) %>%
    bind_cols(Predicted_Sales = pred_values)
write.csv(output_df_results, "XGBoost_output.csv", row.names = F)

# Output results + Training Data as CSV
output_df_all <- df %>%
    dplyr::select(1:4) %>%
    left_join(output_df_results)
write.csv(output_df_all, "XGBoost_output_all.csv", row.names = F)


# Plot Trees
xgb.plot.multi.trees(feature_names = dimnames(salesInfo_matrix)[[2]], model = xgb_model_3)


xgb.plot.multi.trees(feature_names = names(salesInfo_matrix), model = xgb_model_2)



# Plot Importance
importance_matrix <- xgb.importance(names(salesInfo_matrix), model = xgb_model_2)
importance_matrix <- importance_matrix %>%
  arrange(desc(Importance)) %>%
  head(20)
xgb.plot.importance(importance_matrix)
```

Prophet
```{r}
# Too many records for Prophet to run on current machine
# Aggregate data at the Dept level to get Storewide Sales
df_agg_stores <- df[c(1, 3, 4)]
setDT(df_agg_stores)
df_agg_stores <- df_agg_stores[, sum(Weekly_Sales), by = list(Store, Date)]
df_agg_stores <- tibble(df_agg_stores)
df_agg_stores <- df_agg_stores %>%
  arrange(Date) %>%
  dplyr::rename(Storewide_Sales = V1)

df_train <- df_agg_stores[df_agg_stores$Date < as.Date("2012-02-04"),]
df_prophet <- data.frame(ds = df_train$Date, y = df_train$Storewide_Sales)

future <- data.frame(ds = df_agg_stores$Date)
dim(df_prophet)
dim(future)

df_prophet$Store <- df_train$Store

m <- prophet(yearly.seasonality=TRUE)
m <- add_regressor(m, "Store")
m <- fit.prophet(m, df_prophet)

future$Store <- df_agg_stores$Store

forecast <- predict(m, future)
output <- cbind(forecast[c('ds', 'yhat')], df_agg_stores) %>%
  dplyr::select(c(4, 3, 5, 2)) %>%
  dplyr::rename(Predicted_Sales = yhat, Actual_Sales = Storewide_Sales)


# Test forecast
predictions <- output[output$Date > as.Date("2012-02-04"),]
pred_values <- predictions$Predicted_Sales
actual_values <- predictions$Actual_Sales
rmse = caret::RMSE(actual_values, pred_values)
cat(" RMSE: ", rmse)
# RMSE: 507580

# Root Mean Squared Error & Percentage of cases with less than 30% error
errors <- pred_values - actual_values
rel_change <- 1 - ((actual_values - abs(errors)) / actual_values)
pred30 <- table(rel_change<0.30)["TRUE"] / length(actual_values)
paste("PRED(30):", pred30)
# PRED(30): 0.421052631578947

# Output results + Training Data as CSV
write.csv(predictions, "Prophet_output.csv", row.names = FALSE)
```

